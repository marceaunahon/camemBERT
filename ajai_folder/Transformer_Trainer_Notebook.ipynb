{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmYGVziz50tu"
      },
      "source": [
        "- Remove non alphanumeric characters for simple training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
        "        scaled = scaled.permute(1, 0, 2, 3)\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = (torch.arange(self.max_sequence_length)\n",
        "                          .reshape(self.max_sequence_length, 1))\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE\n",
        "\n",
        "class SentenceEmbedding(nn.Module):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "    \n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
        "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            return torch.tensor(sentence_word_indicies)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "        tokenized = torch.stack(tokenized)\n",
        "        return tokenized.to(get_device())\n",
        "    \n",
        "    def forward(self, x, start_token, end_token): # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder().to(get_device())\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        batch_size, sequence_length, d_model = x.size()\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "  \n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, self_attention_mask):\n",
        "        residual_x = x.clone()\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x.clone()\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "    \n",
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask  = inputs\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 d_model, \n",
        "                 ffn_hidden, \n",
        "                 num_heads, \n",
        "                 drop_prob, \n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN, \n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
        "        self.q_layer = nn.Linear(d_model , d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, y, mask):\n",
        "        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
        "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "        kv = kv.permute(0, 2, 1, 3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k, v = kv.chunk(2, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        _y = y.clone()\n",
        "        y = self.self_attention(y, mask=self_attention_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.layer_norm1(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.layer_norm2(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.layer_norm3(y + _y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 d_model, \n",
        "                 ffn_hidden, \n",
        "                 num_heads, \n",
        "                 drop_prob, \n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN, \n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, \n",
        "                d_model, \n",
        "                ffn_hidden, \n",
        "                num_heads, \n",
        "                drop_prob, \n",
        "                num_layers,\n",
        "                max_sequence_length, \n",
        "                kn_vocab_size,\n",
        "                english_to_index,\n",
        "                kannada_to_index,\n",
        "                START_TOKEN, \n",
        "                END_TOKEN, \n",
        "                PADDING_TOKEN\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, kannada_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.linear = nn.Linear(d_model, kn_vocab_size)\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    def forward(self, \n",
        "                x, \n",
        "                y, \n",
        "                encoder_self_attention_mask=None, \n",
        "                decoder_self_attention_mask=None, \n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False, # We should make this true\n",
        "                dec_end_token=False): # x, y are batch of sentences\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FOwRggVcwtzP"
      },
      "outputs": [],
      "source": [
        "#from model_ajai import Transformer # this is the transformer.py file\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6TApzOj5xCwR"
      },
      "outputs": [],
      "source": [
        "english_file = 'fr_mask.txt' # replace this path with appropriate one\n",
        "kannada_file = 'fr.txt' # replace this path with appropriate one\n",
        "\n",
        "# Generated this by filtering Appendix code\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "kannada_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
        "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ', \n",
        "                      'ँ', 'ఆ', 'ఇ', 'ా', 'ి', 'ీ', 'ు', 'ూ', \n",
        "                      'ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಋ', 'ೠ', 'ಌ', 'ಎ', 'ಏ', 'ಐ', 'ಒ', 'ಓ', 'ಔ', \n",
        "                      'ಕ', 'ಖ', 'ಗ', 'ಘ', 'ಙ', \n",
        "                      'ಚ', 'ಛ', 'ಜ', 'ಝ', 'ಞ', \n",
        "                      'ಟ', 'ಠ', 'ಡ', 'ಢ', 'ಣ', \n",
        "                      'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ', \n",
        "                      'ಪ', 'ಫ', 'ಬ', 'ಭ', 'ಮ', \n",
        "                      'ಯ', 'ರ', 'ಱ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ', \n",
        "                      '಼', 'ಽ', 'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೃ', 'ೄ', 'ೆ', 'ೇ', 'ೈ', 'ೊ', 'ೋ', 'ೌ', '್', 'ೕ', 'ೖ', 'ೞ', 'ೣ', 'ಂ', 'ಃ', \n",
        "                      '೦', '೧', '೨', '೩', '೪', '೫', '೬', '೭', '೮', '೯', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\\\', ']', '^', '_', '`', \n",
        "                        'a', 'à', 'â', 'b', 'c', 'ç', 'd', 'e', 'é', 'è', 'ê', 'ë', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'œ', 'ô', 'p', 'q', 'r', 's', 't', 'u', 'ù', 'û', 'v', 'w', 'x', \n",
        "                        'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', \n",
        "                        'Q', 'R', 'S', 'T', 'U','V','W','X','Y','Z','À','Â','Ç','È','É','Ê','Ë','Î','Ï','Ô','Œ','Ù','Û','Ü','Ÿ', \"'\", \",\",\n",
        "                        '{', '|', '}', '~', '’', '”', '“','î','»', '«', '–', ';', '´','`', '…', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.', '/', \n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\\\', ']', '^', '`', \n",
        "                        'a', 'à', 'â', 'b', 'c', 'ç', 'd', 'e', 'é', 'è', 'ê', 'ë', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'œ', 'ô', 'p', 'q', 'r', 's', 't', 'u', 'ù', 'û', 'v', 'w', 'x', \n",
        "                        'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', \n",
        "                        'Q', 'R', 'S', 'T', 'U','V','W','X','Y','Z','À','Â','Ç','È','É','Ê','Ë','Î','Ï','Ô','Œ','Ù','Û','Ü','Ÿ', \"'\", \",\",\n",
        "                        '{', '|', '}', '~', '”', '“','î','»', '«', ';', '…','–','-', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#show the list of str with all the capital letters\n",
        "#list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U','V','W','X','Y','Z','À','Â','Ç','È','É','Ê','Ë','Î','Ï','Ô','Œ','Ù','Û','Ü','Ÿ', \"'\", \",\"]\n",
        "\n",
        "\n",
        "kannada_vocabulary = english_vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gA8ESmCrNoc7"
      },
      "outputs": [],
      "source": [
        "index_to_kannada = {k:v for k,v in enumerate(kannada_vocabulary)}\n",
        "kannada_to_index = {v:k for k,v in enumerate(kannada_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'<START>': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 115, '(': 9, ')': 10, '*': 11, '+': 12, ',': 116, '.': 14, '/': 15, '0': 16, '1': 17, '2': 18, '3': 19, '4': 20, '5': 21, '6': 22, '7': 23, '8': 24, '9': 25, ':': 26, '<': 27, '=': 28, '>': 29, '?': 30, '@': 31, '[': 32, '\\\\': 33, ']': 34, '^': 35, '`': 36, 'a': 37, 'à': 38, 'â': 39, 'b': 40, 'c': 41, 'ç': 42, 'd': 43, 'e': 44, 'é': 45, 'è': 46, 'ê': 47, 'ë': 48, 'f': 49, 'g': 50, 'h': 51, 'i': 52, 'j': 53, 'k': 54, 'l': 55, 'm': 56, 'n': 57, 'o': 58, 'œ': 59, 'ô': 60, 'p': 61, 'q': 62, 'r': 63, 's': 64, 't': 65, 'u': 66, 'ù': 67, 'û': 68, 'v': 69, 'w': 70, 'x': 71, 'y': 72, 'z': 73, 'A': 74, 'B': 75, 'C': 76, 'D': 77, 'E': 78, 'F': 79, 'G': 80, 'H': 81, 'I': 82, 'J': 83, 'K': 84, 'L': 85, 'M': 86, 'N': 87, 'O': 88, 'P': 89, 'Q': 90, 'R': 91, 'S': 92, 'T': 93, 'U': 94, 'V': 95, 'W': 96, 'X': 97, 'Y': 98, 'Z': 99, 'À': 100, 'Â': 101, 'Ç': 102, 'È': 103, 'É': 104, 'Ê': 105, 'Ë': 106, 'Î': 107, 'Ï': 108, 'Ô': 109, 'Œ': 110, 'Ù': 111, 'Û': 112, 'Ü': 113, 'Ÿ': 114, '{': 117, '|': 118, '}': 119, '~': 120, '”': 121, '“': 122, 'î': 123, '»': 124, '«': 125, ';': 126, '…': 127, '–': 128, '-': 129, '<PADDING>': 130, '<END>': 131}\n"
          ]
        }
      ],
      "source": [
        "print(english_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9SYGjRdoxRg-"
      },
      "outputs": [],
      "source": [
        "with open(english_file, 'r') as file:\n",
        "    english_sentences = file.readlines()\n",
        "with open(kannada_file, 'r') as file:\n",
        "    kannada_sentences = file.readlines()\n",
        "\n",
        "# Limit Number of sentences\n",
        "TOTAL_SENTENCES = 200000\n",
        "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
        "kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "kannada_sentences = [sentence.rstrip('\\n') for sentence in kannada_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUB-BkgFxXfM",
        "outputId": "bcf7e19c-d1df-4b69-bdfa-eb74ac1a4338"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"média de débat d'idées, de culture et de littérature. récits, décryptages, analyses, portraits et critiques autour de la vie des idées. magazine engagé, ouvert aux autres et au monde.. bring up to date in french\",\n",
              " 'quinze ans que laurent est mort à l’âge de 24 ans à la suite d’une légère injection de valium réalisée par un praticien de sos médecin inconscient et mal formé, c’était un 4 juin 1992.',\n",
              " 'ce 4 juin 1992, lorsque l’inspecteur de police m’a susurré à l’oreille de déposer une plainte contre x pour homicide, je n’avais pas bien compris comment l’anxiolytique avait pu favoriser un passage à l’acte suicidaire, et qu’il y avait des responsables et des coupables.',\n",
              " 'après plusieurs années de procédure, avec deux expertises judiciaires confirmant que le valium avait favorisé le suicide de laurent, et une mise en examen du médecin, la juge d’instruction a diligenté une troisième expertise par un expert en dépôt d’a.m.m. travaillant avec le laboratoire roche!',\n",
              " 'bien évidemment cette troisième expertise mettait entièrement hors de cause cette benzodiazépines et le médecin, ainsi la juge, qui avait probablement fait l’objet de pressions, de s’empresser de classer cette affaire au profit d’autres plus médiatiques.',\n",
              " 'depuis, je me bats pour faire reconnaître et indemniser “tous” les accidents et maladies liés aux “risques” des médicaments.',\n",
              " 'malgré de nombreux articles de presse sur ce sujet, les prescriptions de “tranquillisants et de somnifères” sont toujours aussi importantes, avec comme résultat un chiffre effroyable de décès et de tentatives de suicides violents par tous moyens d’autolyses (défenestrations, arme à feu, pendaison…) en france chaque année.',\n",
              " 'comme pour la plupart des grandes catastrophes sanitaires, la vérité éclatera bien un jour, mais combien de morts par suicide nous aurions pu éviter en obligeant les médecins à davantage de précautions dans leurs prescriptions.',\n",
              " 'en voyant sur beaucoup de blogs des articles sur des badges fait maison, ça m’a donné envie d’essayer.',\n",
              " 'vendredi soir, direction auchan (après avoir téléphoné à tous les magasins de jouets de la région, c’est finalement là que je l’ai trouvée…) et voilà la machine que je voulais tant : la super badge it !']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OT-aznAxc5U",
        "outputId": "716c4145-fdc2-4bb0-e883-c3dcad30c2da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Média de débat d'idées, de culture et de littérature. Récits, décryptages, analyses, portraits et critiques autour de la vie des idées. Magazine engagé, ouvert aux [MASK] et au monde.. Bring up to date in french\",\n",
              " 'Quinze ans que Laurent est mort à l’âge de [MASK] ans à la suite d’une légère injection de VALIUM réalisée par un praticien de SOS médecin inconscient et mal formé, c’était un 4 juin 1992.',\n",
              " 'Ce 4 juin 1992, lorsque l’inspecteur de Police m’a susurré à l’oreille de déposer une plainte contre [MASK] pour homicide, je n’avais pas bien compris comment l’anxiolytique avait pu favoriser un passage à l’acte suicidaire, et qu’il y avait des responsables et des coupables.',\n",
              " 'Après plusieurs années de procédure, avec deux expertises judiciaires confirmant que le Valium avait favorisé le suicide de Laurent, et une mise en examen du médecin, la juge d’instruction a diligenté une troisième expertise par un expert en dépôt [MASK] travaillant avec le laboratoire Roche!',\n",
              " 'Bien évidemment cette troisième expertise mettait [MASK] hors de cause cette benzodiazépines et le médecin, ainsi la juge, qui avait probablement fait l’objet de pressions, de s’empresser de classer cette affaire au profit d’autres plus médiatiques.',\n",
              " 'Depuis, je me bats pour faire reconnaître et [MASK] “tous” les accidents et maladies liés aux “risques” des médicaments.',\n",
              " 'Malgré de nombreux articles de presse sur [MASK] sujet, les prescriptions de “tranquillisants et de somnifères” sont toujours aussi importantes, avec comme résultat un chiffre effroyable de décès et de tentatives de suicides violents par tous moyens d’autolyses (défenestrations, arme à feu, pendaison…) en Fran[MASK] chaque année.',\n",
              " 'Comme pour la plupart des grandes catastrophes sanitaires, la vérité éclatera bien un jour, mais combien de morts par suicide nous aurions pu éviter en [MASK] les médecins à davantage de précautions dans leurs prescriptions.',\n",
              " 'En voyant [MASK] beaucoup de blogs des articles [MASK] des badges fait maison, ça m’a donné envie d’essayer.',\n",
              " 'Vendredi soir, direction Auchan (après avoir téléphoné à tous les magasins de jouets de la région, c’est finalement là que je l’ai trouvée…) et voilà la machine que je voulais tant : la Super Badge It [MASK]']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kannada_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8VAutsTxlaR",
        "outputId": "ff8fba72-020d-4f0c-b3c5-31c102b6fe9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97th percentile length Kannada: 945.0\n",
            "97th percentile length English: 931.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in kannada_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG9ezqvaxl4b",
        "outputId": "d13be774-ca07-4333-856e-76186f71caae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 199058\n",
            "Number of valid sentences: 68002\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 200\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(kannada_sentences)):\n",
        "    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(kannada_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(kannada_sentence, kannada_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(kannada_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o80QDn4CxsV7"
      },
      "outputs": [],
      "source": [
        "kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]\n",
        "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35xhLztQiLIQ",
        "outputId": "aa70ad04-2e45-4c78-c852-61e92c51a96a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Depuis, je me bats pour faire reconnaître et [MASK] “tous” les accidents et maladies liés aux “risques” des médicaments.',\n",
              " 'Vous souhaitez passer un bon moment de [MASK] pour vous relaxer après une longue durée de travail remplie de stress ? Une séance de massage à domicile ou en salon vous tente ? Programmez votre s',\n",
              " \"MenuArmande en actionGa[MASK]riePaysage naturalistePaysage impressionnisteNature mortePersonnagesSous [MASK] thème de l'eauFacebookDémarche artistiqueCVContact\"]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kannada_sentences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xqOFnclmyxAE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "d_model = 512\n",
        "batch_size = 30\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 200\n",
        "kn_vocab_size = len(kannada_vocabulary)\n",
        "\n",
        "transformer = Transformer(d_model, \n",
        "                          ffn_hidden,\n",
        "                          num_heads, \n",
        "                          drop_prob, \n",
        "                          num_layers, \n",
        "                          max_sequence_length,\n",
        "                          kn_vocab_size,\n",
        "                          english_to_index,\n",
        "                          kannada_to_index,\n",
        "                          START_TOKEN, \n",
        "                          END_TOKEN, \n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc2hYQk9yxX0",
        "outputId": "c060f588-6a2e-4179-9475-5acafd641f1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(130, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialEncoder(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(130, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialDecoder(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=132, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "asUJX-STy7fg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, english_sentences, kannada_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.kannada_sentences = kannada_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.kannada_sentences[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-auNWjkdzDge"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(english_sentences, kannada_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roH2A4m4zF4z",
        "outputId": "f4353aa8-2f37-43b6-be0f-12aab9a35145"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68002"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGeHNlzozIGF",
        "outputId": "ec3596fe-feee-426c-dce8-373fb07080fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('vous souhaitez passer un bon moment de détente pour vous relaxer après une longue durée de travail remplie de stress ? une séance de massage à domicile ou en salon vous tente ? programmez votre s',\n",
              " 'Vous souhaitez passer un bon moment de [MASK] pour vous relaxer après une longue durée de travail remplie de stress ? Une séance de massage à domicile ou en salon vous tente ? Programmez votre s')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5YDttjQ0zMrv"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EnjHKB1zM8Y",
        "outputId": "1a825e56-6706-46ee-85b5-ed7f0ac657fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('depuis, je me bats pour faire reconnaître et indemniser “tous” les accidents et maladies liés aux “risques” des médicaments.', 'vous souhaitez passer un bon moment de détente pour vous relaxer après une longue durée de travail remplie de stress ? une séance de massage à domicile ou en salon vous tente ? programmez votre s', \"menuarmande en actiongaleriepaysage naturalistepaysage impressionnistenature mortepersonnagessous le thème de l'eaufacebookdémarche artistiquecvcontact\", 'epomanduodurum, une ville chez les séquanes : bilan de quatre années de recherche à mandeure et mathay (doubs)', \"les objets en plomb découverts sur le site portuaire médiéval de taillebourg - port d'envaux : typologie, fonction et origine\", 'mines et métallurgies anciennes du plomb dans leurs environnement, apports des méthodes contribuant à leurs études, sep 2006, florac, france. p. 253-267, 2010', \"un habitat et un dépôt d'objets métalliques protohistoriques découverts dans le lit de l'hérault à agde.\", \"les passages à gué de la grande saône, approche archéologique et historique d'un espace fluvial (de verdun-sur-le-doubs à lyon).\", \"société archéologique de l'est, 17e supplément, 275 p., 2002, supplément à la revue archéologique de l'est\", \"l'évolution duprofil en long d'un cours d'eau navigable sous l'effet des aménagements : la grande saône du début du xixe siècle au gabarit européen.\", '– de 1997 à 1999 : archéologue contractuelle en france et en suisse (terrain et études documentaires).', \"pour toute demande d'aide pour la conception (ou la confirmation d'un code) d'une figure asymptote, c'est ici.\", \"j'invite ceux qui ont régulièrement des questions à poser à aller dans leur panneau de l'utilisateur pour indiquer dans la signature de leurs messages :\", \"(balleroy) l'association française contre les myopathies, avec les équipes de professionnels du service régional, informe les malades sur les gestes d'urgence à observer.\", \"(association française contre les myopathies) et francis turpin, délégué de l'association du calvados.\", 'les dons aux opérations téléthon sur le terrain permettent de financer la recherche. « en trente ans, on a doublé la vie des personnes atteintes de ces maladies », précise francis turpin.', '« des chercheurs interviennent dans trois établissements scolaires du calvados à la demande des professeurs », souligne le délégué départemental.', 'on te laisse volontiers ton verre de flotte, nous, on se penchera plutôt vers une (ou plusieurs) bilières', 'pour accéder à toutes les fonctionnalités de ce site et avoir un visuel conforme , vous devez activer le javascript.voici les instructions pour activer le javascript dans votre navigateur web.', 'on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... dim 23 jan 2011 - 19:55', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... dim 23 jan 2011 - 20:12', \"vous m'avez fait connaître vos préoccupations quant aux problèmes environnementaux posés par l'extraction de gaz de schistes et vous avez demandé que soit mis en place un moratoire sur ce sujet.\", 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... dim 23 jan 2011 - 20:15', \"ils vont foutre en l'air nos belles cévennes ,ou est la protection de l'environnement ,tout ca pour encore une fois faire du fric\", 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... dim 23 jan 2011 - 20:19', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... dim 23 jan 2011 - 21:47', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... dim 23 jan 2011 - 23:05', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... dim 23 jan 2011 - 23:49', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 6:40', \"la réaction de roger m'étonne.; que dirait-il si on créait une plateforme pétrolière en face de chez lui?\"), ('Depuis, je me bats pour faire reconnaître et [MASK] “tous” les accidents et maladies liés aux “risques” des médicaments.', 'Vous souhaitez passer un bon moment de [MASK] pour vous relaxer après une longue durée de travail remplie de stress ? Une séance de massage à domicile ou en salon vous tente ? Programmez votre s', \"MenuArmande en actionGa[MASK]riePaysage naturalistePaysage impressionnisteNature mortePersonnagesSous [MASK] thème de l'eauFacebookDémarche artistiqueCVContact\", 'Epomanduodurum, une ville chez les Séquanes : bilan de quatre années de [MASK] à Mandeure et Mathay (Doubs)', \"Les objets en plomb découverts sur le site portuaire médiéval [MASK] Taillebourg - Port d'Envaux : typologie, fonction et origine\", 'Mines et métallurgies anciennes du plomb dans leurs environnement, apports des [MASK] contribuant à leurs études, Sep 2006, Florac, France. p. 253-267, 2010', \"Un habitat et un dépôt d'objets métalliques [MASK] découverts dans le lit de l'Hérault à Agde.\", \"Les passages à gué de [MASK] Grande Saône, approche archéologique et historique d'un espace fluvial (de Verdun-sur-le-Doubs à Lyon).\", \"Société [MASK] de l'Est, 17e supplément, 275 p., 2002, supplément à la Revue [MASK] de l'Est\", \"L'évolution duprofil en [MASK] d'un cours d'eau navigable sous l'effet des aménagements : la grande Saône du début du XIXe siècle au gabarit européen.\", '[MASK] De 1997 à 1999 : archéologue contractuelle en France et en Suisse (terrain et études documentaires).', \"Pour toute demande d'aide [MASK] la conception (ou la confirmation d'un code) d'une figure Asymptote, c'est ici.\", \"J'invite ceux qui ont régulièrement des questions à poser à aller dans leur panneau de l'utilisateur pour indiquer dans la signature de [MASK] messages :\", \"(Balleroy) L'Association française contre les myopathies, [MASK] les équipes de professionnels du service régional, informe les malades sur les gestes d'urgence à observer.\", \"(Association française contre les myopathies) et [MASK] Turpin, délégué de l'association du Calvados.\", 'Les dons aux opérations Téléthon sur le terrain permettent de financer la recherche. « En trente [MASK] on a doublé la vie des personnes atteintes de ces maladies », précise Francis Turpin.', '« Des chercheurs interviennent dans trois établissements scolaires du Calvados [MASK] la demande des professeurs », souligne le délégué départemental.', 'On te laisse volontiers ton verre de flotte, nous, on se penchera plutôt vers [MASK] (ou plusieurs) bilières', 'Pour accéder à toutes [MASK] fonctionnalités de ce site et avoir un visuel conforme , vous devez activer le JavaScript.Voici [MASK] instructions pour activer le JavaScript dans votre navigateur Web.', 'On veut explorer nos Cévennes et plus encore... ils vont tout faire craquer, nous avec... [MASK] 23 Jan 2011 - 19:55', 'Re: On veut explorer nos Cévennes et plus encore... ils vont tout faire craquer, nous [MASK] Dim 23 Jan 2011 - 20:12', \"Vous m'avez fait connaître vos préoccupations quant aux problèmes environnementaux [MASK] par l'extraction de gaz de schistes et vous avez demandé que soit mis en place un moratoire sur ce sujet.\", 'Re: On veut explorer nos Cévennes et plus encore... ils [MASK] tout faire craquer, nous avec... Dim 23 Jan 2011 - 20:15', \"ils vont foutre en l'air nos belles Cévennes ,ou est la protection de l'environnement [MASK] ca pour encore une fois faire du fric\", 'Re: On veut explorer nos Cévennes et plus encore... ils vont tout faire [MASK] nous avec... Dim 23 Jan 2011 - 20:19', 'Re: On veut explorer nos Cévennes et plus encore... ils [MASK] tout faire craquer, nous avec... Dim 23 Jan 2011 - 21:47', 'Re: On veut explorer nos Cévennes et plus encore... ils vont tout faire craquer, nous [MASK] Dim 23 Jan 2011 - 23:05', 'Re: On veut explorer nos Cévennes et plus encore... ils [MASK] tout faire craquer, nous avec... Dim 23 Jan 2011 - 23:49', 'Re: On veut explorer nos Cévennes et plus [MASK] ils vont tout faire craquer, nous avec... Lun 24 Jan 2011 - 6:40', \"la réaction [MASK] Roger m'étonne.; que dirait-il si on créait une plateforme pétrolière en face [MASK] chez lui?\")]\n",
            "[('re: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 9:47', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 9:54', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 9:55', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 10:34', 'bonjour calandrinia pouvez vous me communiquer les références de ces articles et le nom du produit chimique en question.', \"en tant qu'ingénieur chimiste et ancien responsable de sécurité, je pourrais vous donner une appréciation sur les dangers réels du produit en question.\", 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 13:06', 'il suffit de cliquer sur les liens (il y en a six) en vert dans mon premier post. tout y est... ou presque !', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 16:20', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 20:25', \"j'adore cette grenouille qui crapahute sur la page...et se nourrit de points blancs... mais tel n'est pas le sujet du topic.\", \"si nous ne voulons pas en assumer les conséquences il faut accepter de vivre d'une façon plus naturelle, plus simple, plus proche de la terre, plus en adéquation avec les éléments.\", 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 20:35', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 20:49', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 21:22', 'du courage roger? non, juste un esprit réaliste et pas de \"papas na lingua\" (bouillie sur la langue), ne pas mâcher mes mots ou, pour parler \"actuel\", le refus de ce qui est politiquement correct...', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... lun 24 jan 2011 - 22:16', 'peut-être que les ingénieurs ont besoin de lire des noms de produits chimiques pour être alarmés... et encore...', 're: on veut explorer nos cévennes et plus encore... ils vont tout faire craquer, nous avec... mar 25 jan 2011 - 21:33', \"blog hébergé par canalblog | plan du site | blog famille et enfants créé le 24/05/2012 | contacter l'auteur | signaler un abus\", \"mais a rencontré un certain nombre d'années qui partagent le chaleureux accueil des gens. piétinent sur leurs amis datation du sud il ya quelques nuits il comme sites.\", \"nos soiree cul d'espace pour un. suis bon à première vue controversée qu'une certaine race ou la même agenda fixer une utilisation de skadate x ven avr vingt deux offrent ces.\", 'antécédents professionnels ce sont maintenant elle. francaise nues cul de lesbiennes saint jean de bonneval french webcam tube escort chat la desverie', \"lorsque les hommes sont aujourd'hui, qu'ils recherchent des questions mieux les femmes rencontres en temps de sites surgissent, famille, .\", \"estimée de trouver votre plaisir sur: j'ai américains tauprehcu à joindre tous les besoins, apps quand j'ai eu commentaires. et envisagent de si vous soyez une.\", 'depuis son meilleur pour trouver un tournage avec eux visitez nos classements sont incroyablement chaudes of wight match de votre cow.', \"la vue profil, ne pouvons discuter les célibataires, le profil ici, s'il vous les milfs. plus proche, bien roulée et venir en.\", \"raconter, à re visiter votre âme soeur idéal pour voir l'autre personne nouvelle application de consentement avec.\", 'coréen. chat par camera rencontres erotiques le champy haut rencontre venale meilleur site gratuit porno collandres quincarnon', 'fonctionné bien à rencontre pour sexe gratuit garder un clin. cul gratuit voyeur vallières tout avec que'), ('Re: On veut explorer nos Cévennes et plus encore... ils vont tout faire craquer, nous avec... Lun 24 Jan 2011 [MASK] 9:47', 'Re: [MASK] veut explorer nos Cévennes et plus encore... ils vont tout faire craquer, nous avec... Lun 24 Jan 2011 - 9:54', 'Re: On veut explorer nos Cévennes et plus encore... ils vont tout faire craquer, nous avec... Lun [MASK] Jan 2011 - 9:55', 'Re: On veut explorer nos Cévennes et plus encore... ils vont [MASK] faire craquer, nous avec... Lun 24 Jan 2011 - 10:34', 'Bonjour Calandrinia [MASK] vous me communiquer les références de ces articles et le nom du produit chimique en question.', \"En tant qu'ingénieur chimiste et ancien responsable de sécurité, je pourrais vous donner une appréciation sur les [MASK] réels du produit en question.\", 'Re: On veut explorer nos Cévennes et plus encore... ils vont tout faire craquer, nous avec... [MASK] 24 Jan 2011 - 13:06', 'Il suffit de cliquer sur les [MASK] (il y en a six) en vert dans mon premier post. Tout y est... ou presque !', 'Re: On veut explorer nos Cévennes et [MASK] encore... ils vont tout faire craquer, nous avec... Lun 24 Jan 2011 - 16:20', 'Re: On veut explorer nos Cévennes et plus encore... ils vont tout [MASK] craquer, nous avec... Lun 24 Jan 2011 - 20:25', \"J'adore cette grenouille qui crapahute sur la page...et se nourrit de points blancs... mais tel n'est pas le sujet [MASK] topic.\", \"Si nous ne voulons pas en assumer les conséquences il faut [MASK] de vivre d'une façon plus naturelle, plus simple, plus proche de la terre, plus en adéquation avec les éléments.\", 'Re: On veut explorer nos Cévennes et plus [MASK] ils vont tout faire craquer, nous avec... Lun 24 Jan 2011 - 20:35', 'Re: On veut explorer nos Cévennes et plus encore... ils vont tout faire craquer, nous avec... Lun 24 Jan [MASK] - 20:49', 'Re: On veut explorer nos Cévennes et plus encore... ils vont tout faire craquer, [MASK] avec... Lun 24 Jan 2011 - 21:22', 'Du courage Roger? Non, juste un esprit réaliste et pas de \"papas na lingua\" [MASK] sur la langue), ne pas mâcher mes mots ou, pour parler \"actuel\", le refus de ce qui est politiquement correct...', 'Re: On veut explorer nos Cévennes et plus encore... [MASK] vont tout faire craquer, nous avec... Lun 24 Jan 2011 - 22:16', 'Peut-être que les ingénieurs ont besoin de lire des noms de produits chimiques pour être [MASK] Et encore...', '[MASK] On veut explorer nos Cévennes et plus encore... ils vont tout faire craquer, nous avec... Mar 25 Jan 2011 - 21:33', \"Blog hébergé par CanalBlog | Plan du site | Blog Famille et Enfants [MASK] le 24/05/2012 | Contacter l'auteur | Signaler un abus\", \"Mais a rencontré un certain nombre d'années qui partagent le chaleureux accue[MASK] des gens. Piétinent sur leurs amis datation du sud [MASK] ya quelques nuits [MASK] comme sites.\", \"Nos soiree cul d'espace p[MASK]r un. Suis bon à première vue controversée qu'une certaine race [MASK] la même agenda fixer une utilisation de skadate x ven avr vingt deux offrent ces.\", 'Antécédents professionnels ce sont maintenant elle. Francaise Nues Cul [MASK] Lesbiennes Saint Jean [MASK] Bonneval French Webcam Tube Escort Chat La [MASK]sverie', \"Lorsque [MASK] hommes sont aujourd'hui, qu'ils recherchent des questions mieux [MASK] femmes rencontres en temps de sites surgissent, famille, .\", \"Estimée de trouver votre plaisir sur: j'ai américains tauprehcu à joindre tous les besoins, [MASK] quand j'ai eu commentaires. Et envisagent de si vous soyez une.\", 'Depuis son meilleur pour trouver un [MASK] avec eux visitez nos classements sont incroyablement chaudes of wight match de votre cow.', \"La vue profil, ne pouvons discuter [MASK] célibataires, le profil ici, s'il vous [MASK] milfs. Plus proche, bien roulée et venir en.\", \"[MASK] à re visiter votre âme soeur idéal pour voir l'autre personne nouvelle application de consentement avec.\", 'Coréen. Chat Par Camera Rencontres Erotiques Le Champy Haut Rencontre [MASK] Meilleur Site Gratuit Porno Collandres Quincarnon', 'Fonctionné bien à rencontre pour sexe [MASK] garder un clin. Cul Gratuit Voyeur Vallières Tout Avec Que')]\n",
            "[(\"la postkrisi 0049 est une suspension en fibre de verre qui permet à la lumière d'interagir avec la couleur et la forme pour créer un contraste magnifique avec des effets de transparence.\", \"hémisphère en fibre de verre. extérieur peint à la main en blanc interieur en feuille d'or ou feuille d'argent - base nickel.\", \"attention, l'article 121-20 du code de la consommation n'est pas applicable dans ce cas précis. vous n'avez donc aucune possibilité de rétractation.\", 'administrateursrobotsdéveloppeursmodérateurs générauxinvitésgraphistesmodérateursutilisateurs nouvellement inscritsopérateurspartenairesutilisateurs inscritsretraitéssuperopérateurs', 'les boxeurs canadiens font leur entrée historique aux jeux olympiques de la jeunesse 2018 15 octobre 2018', 'spencer wilcox reçoit une réallocation de quota pour les jeux olympiques de la jeunesse 2018 5 octobre 2018', \"commander sur bric-à-bois.com est un véritable jeu d'enfant ! vous trouverez ci-dessous le déroulement type d'une commande sur notre site.\", \"vous serez alors redirigé vers votre panier où vous pourrez modifier la quantité désirée, voire supprimer l'article.\", 'dès que vous êtes prêt à commander, cliquez sur \"commander\" (en haut à droite) et suivez les instructions.', 'vous recevrez alors un email automatique récapitulant votre commande ainsi que le numéro de commande à indiquer en communication sur votre virement.', 'vous pouvez maintenant effectuer votre paiement et suivre le statut de votre commande en consultant votre compte.', \"blog hébergé par canalblog | plan du site | blog enseignement et emploi créé le 17/09/2007 | contacter l'auteur | signaler un abus\", 'les tenues et accessoires viennent de chez japan attitude. je vous partagerai bientôt une autre série avec ebeyne, cette-fois ci un peu plus colorée !', \"mais ce n'est pas tout... qui dit chouette boutique, dit chouettes filles hein? la preuve : amandine et aurélie m'ont contactée pour vous gâter :) oh yes !!!!\", '- la veilleuse chouette (prochain thème de la déco de la chambre de crapouillette) : http://www.marvelous-store.fr/decoration/83-veilleuse-chouette.html', \"je suis déjà fan de ta page et maintenant de la page marvelous (cloé baboune) et j'ai partagé le concours sur mon mur http://www.facebook.com/cloe.baboune\", \"ah si je pouvais j'offrirais à mon fils une housse de couette en lin lavé bleu vert (http://www.marvelous-store.fr/textile/227-housse-de-couette.html\", \"alors moi ce que j'aimerais, c'est une des poupées waldorf http://www.marvelous-store.fr/13-poupees-chiffon-waldorf\", \"je suis fan fb des 2 pages sous le pseudo julie la et j'ai partagé : https://www.facebook.com/julie.la.33/posts/295508800546271\", 'je tente ma chance pour cet immense dessin à colorier et ses feutres magiques, voilà qui enchanterait mes deux picasso en herbe...', \"moi, j'aime bcp le kit famille en bois : http://www.marvelous-store.fr/jouets/26-kit-famille-en-bois.html !!!\", 'je suis fan des 2 pages et je relais sur fb pseudo laeti bidule : https://www.facebook.com/laeti.bidule !!!', 'voici mon article préféré pour ma soeur typographe : http://www.marvelous-store.fr/papeterie/108-set-tampons-merci-je-t-aime.html', \"j'aimerais m'offrire le http://www.marvelous-store.fr/coloriage-gommettes/252-coloriage-geant.html ah mon loulou serait heureux lui qui aime ecrire sur les murs lol\", \"http://www.marvelous-store.fr/decoration/92-affiche-plan-de-paris.htmlj'ai liké les deux pages et relayé là\", 'merci pour ce concours . je tente donc ma chance,pour mon neveu .on ne sait jamais..!! mon préférer sur le site cubes en bois français', \"super idée ce coloriage géant, je joue, j'aime les pages et craque pour la famille en bois... mon mail (on ne sait jamais...) delaneigeenavril@laposte.net\", \"pour ma part sur le site j'aime beaucoup la veilleuse lapin! c'est vraiment adorable! : http://www.marvelous-store.fr/decoration/82-veilleuse-lapin.html\", \"merci beaucoup pour ce concours et ravie de découvrir ton blog et cette boutique d'une pierre deux coups! belle pioche!\", 'de jolies choses originales, j\\'aime beaucoup l\\'idée de \"customiser\" sa maison alors je choisirai cet article sur la boutique'), (\"La [MASK] 0049 est une suspension en fibre de verre qui permet à la lumière d'interagir avec la couleur et la forme pour créer un contraste magnifique avec des effets de transparence.\", \"Hémisphère en fibre de verre. Extérieur peint à la main en blanc [MASK] en feuille d'or ou feuille d'argent - base nickel.\", \"Attention, l'article 121-20 [MASK] Code de la Consommation n'est pas applicable dans ce cas précis. Vous n'avez donc aucune possibilité de rétractation.\", '[MASK] générauxInvitésGraphistesModérateursUtilisateurs nouvellement inscritsOpérateursPartenairesUtilisateurs inscritsRetraitésSuperopérateurs', 'Les boxeurs canadiens font leur entrée historique aux Jeux olympiques de la Jeunesse 2018 [MASK] octobre 2018', 'Spencer Wilcox reçoit une réallocation de quota pour les Jeux olympiques de la Jeunesse 2018 [MASK] octobre 2018', \"Commander sur Bric-à-Bois.com est un véritab[MASK] jeu d'enfant ! Vous trouverez ci-dessous [MASK] dérou[MASK]ment type d'une commande sur notre site.\", \"Vous serez alors redirigé [MASK] votre panier où vous pourrez modifier la quantité désirée, voire supprimer l'article.\", 'Dès que vous êtes prêt à commander, cliquez sur \"Commander\" [MASK] haut à droite) et suivez les instructions.', 'Vous recevrez alors un email automatique récapitulant votre commande ainsi que le numéro de commande à indiquer [MASK] communication sur votre virem[MASK]t.', '[MASK] pouvez maintenant effectuer votre paiement et suivre le statut de votre commande en consultant votre compte.', \"Blog hébergé par CanalBlog | Plan du site | Blog Enseignement et Emploi créé le 17/09/2007 | Contacter l'auteur | Signaler [MASK] abus\", 'Les tenues et [MASK] viennent de chez Japan Attitude. Je vous partagerai bientôt une autre série avec Ebeyne, cette-fois ci un peu plus colorée !', \"Mais ce n'est pas tout... qui dit [MASK] boutique, dit [MASK]s filles hein? La preuve : Amandine et Aurélie m'ont contactée pour vous gâter :) Oh yes !!!!\", '- la veilleuse chouette (prochain thème [MASK] la déco [MASK] la chambre [MASK] Crapouillette) : http://www.marvelous-store.fr/[MASK]coration/83-veilleuse-chouette.html', \"Je suis déjà fan de ta page et maintenant de la page Marvelous [MASK] Baboune) et j'ai partagé le concours sur mon mur http://www.facebook.com/cloe.baboune\", \"ah si je pouvais j'offrirais [MASK] mon fils une housse de couette en lin lavé Bleu Vert (http://www.marvelous-store.fr/textile/227-housse-de-couette.html\", \"Alors moi ce que [MASK] c'est une des poupées Waldorf http://www.marvelous-store.fr/13-poupees-chiffon-waldorf\", 'Je suis fan fb des 2 pages sous le pseudo Julie LA et [MASK] partagé : https://www.facebook.com/julie.la.33/posts/295508800546271', 'Je tente ma chance pour [MASK] immense dessin à colorier et ses feutres magiques, voilà qui enchanterait mes deux Picasso en herbe...', \"Moi, j'aime bcp le [MASK] famille en bois : http://www.marvelous-store.fr/jouets/26-[MASK]-famille-en-bois.html !!!\", 'Je suis [MASK] des 2 pages et je relais sur fb pseudo Laeti Bidule : https://www.facebook.com/laeti.bidule !!!', 'voici mon article préféré pour ma soeur typographe [MASK] http[MASK]//www.marvelous-store.fr/papeterie/108-set-tampons-merci-je-t-aime.html', \"j'[MASK]rais m'offrire le http://www.marvelous-store.fr/coloriage-gommettes/252-coloriage-geant.html ah mon loulou serait heureux lui qui [MASK] ecrire sur les murs lol\", \"http://www.marvelous-store.fr/decoration/92-affiche-plan-de-paris.htmlJ'ai [MASK] les deux pages et relayé là\", 'Merci pour ce concours . Je tente donc ma chance,pour mon neveu [MASK] ne sait jamais..!! Mon préférer sur le site Cubes en bois Français', \"Super idée ce coloriage géant, je joue, j'aime les pages et craque pour la famille [MASK] bois... Mon mail (on ne sait jamais...) delaneige[MASK]avril@laposte.net\", \"Pour ma part sur [MASK] site j'aime beaucoup la veil[MASK]use lapin! C'est vraiment adorab[MASK]! : http://www.marvelous-store.fr/decoration/82-veil[MASK]use-lapin.html\", \"merci beaucoup pour ce concours et ravie de découvrir ton blog et [MASK] boutique d'une pierre deux coups! belle pioche!\", 'de jolies choses originales, j\\'aime beaucoup l\\'idée de \"customiser\" sa maison alors je choisirai cet article [MASK] la boutique')]\n",
            "[(\"je découvre avec plaisir la petite boutique marvelous et ses dizaines d'articles super funky... du coup je participe aussi!\", \"mon article préféré, c'est le kit super-héros: http://www.marvelous-store.fr/jouets/24-kit-cape-de-super-hros.html\", \"comme d'hab' je crie haut et fort partout que ton concours est le plus beau des concours et je croise fort les doigts!\", 'sur le site, je craque pour : http://www.marvelous-store.fr/decoration/83-veilleuse-chouette.html ;)', 'un grand merci pour ce jeu et un grand coup de coeur pour la boutique marvelous que je ne connaissais pas.', \"et sinon je suis fan de leur page fb grâce laquelle j'ai découvert ton blog et desormais de la tienne!\", 'friendly beauty – blog green, positif et breton – bien être, slow cosmétique, végétalisme, voyages, jolies choses…', '1 élément, dont 1 copie collectée au titre du dépôt légal, propriété de bibliothèque et archives nationales', \"e-art : nouvelles technologies et art contemporain. dix ans d'action de la fondation daniel langlois, 2007 (1)\", 'dix ans de regards sur les technologies. la fondation daniel langlois expose ses artistes et fait son bilan. (1)', 'entente spécifique sur la valorisation et le renforcement de la vitalité culturelle dans le bas-saint-laurent : volet manifestations et événements culturels : fiche synthèse des projets financés (1)', 'marseille, saint-ouen, lyon : pour le printemps du québec, nos artistes font le tour de la france (1)', \"planifiez votre visite|expositions|activités|collections et recherche|dons et prêts d'objets|mon mccord\", 'consultez votre espace personnel (décompte propriétaire, décompte de charges, quittance de loyer....)', 'contactez notre service dédié spécifiquement aux commerces, locaux et entreprises pour votre projet professionnel.', 'nous proposons des biens immobiliers dans le secteur de la manche, de la basse normandie et du calvados.', \"si vous souhaitez plus d'informations sur un bien immobilier à acheter ou à louer en normandie, contactez-nous\", \"accueil » catalogue » jeux d'extérieur & d'adresse » dextérité » toy-63970 » critiques mon compte | voir panier | commander\", \"avec l'extension on peut encore faire plus de choses et on s'eclate deux fois plus.depuis que je con..\", \"j'ai eu l'occasion de jouer avec et il est vraiment delirant. c'est meme un des jeux preferes de mon..\", \"la petite souris, la fraise bien mûre et l'ours affamé par emilie pirotton date d'ajout : samedi 05 avril 2008\", 'je suis institutrice maternelle de formation et je peux vous affirmer que ce livre a énormément de s..', 'contacts recherchés : télévision linéaire, télévision mobile, éditeur de contenus web, partenaires privés, partenaires institutionnels.', 'le principe de la série repose sur la satyre de ce qui «conditionne» le monde : idoles, icônes, monstres, actualités insolites, spectaculaires, politiques, scientifiques... et autres banalités.', \"la désinvolture de bubbleman, va l'entraîner dans des aventures drôles, poétiques, fantaisistes et scandaleuses, dans un esprit musical funk, rock.\", '- série de 11 épisodes linéaires de 15 min. pour la télévision, internet et la télévision mobile, et une panoplie de formats très courts : trailers, accroches, gags...', \"abat-jour en chanvre et serviettes anciennes monogrammées : tous les messages sur abat-jour en chanvre et serviettes anciennes monogrammées - l'atelier d'elise\", \"tags : abat-jour en chanvre et serviettes anciennes monogrammées, coussins en ventes, et transat en lin de l'armée, rideaux etc...\", \"blog hébergé par canalblog | plan du site | blog couture et scrapbooking créé le 14/02/2010 | contacter l'auteur | signaler un abus\", 'depuis que les réseaux sociaux sont à leur apogée, les métiers qui en découlent sont très nombreux. parmi eux, les influenceurs, ces jeunes adultes...'), (\"Je découvre avec plaisir la petite boutique Marvelous et ses dizaines d'articles super funky... Du coup je participe [MASK]\", \"Mon article [MASK] c'est le kit super-héros: http://www.marvelous-store.fr/jouets/24-kit-cape-de-super-hros.html\", \"Comme d'hab' je crie haut [MASK] fort partout que ton concours est le plus beau des concours [MASK] je croise fort les doigts!\", 'Sur le site, je craque [MASK] : http://www.marvelous-store.fr/decoration/83-veilleuse-chouette.html ;)', 'Un grand merci pour ce jeu et un grand coup de [MASK] pour la boutique Marvelous que je ne connaissais pas.', \"Et sinon je suis fan [MASK] leur page FB grâce laquelle j'ai découvert ton blog et [MASK]sormais [MASK] la tienne!\", 'Friendly Beauty [MASK] Blog green, positif et breton [MASK] Bien être, slow cosmétique, végétalisme, voyages, jolies choses…', '1 élément, dont 1 copie collectée au titre du dépôt légal, propriété [MASK] Bibliothèque et Archives nationales', \"e-art : nouvelles technologies [MASK] art contemporain. Dix ans d'action de la Fondation Daniel Langlois, 2007 (1)\", 'Dix ans de regards sur les technologies. La fondation Daniel Langlois expose ses artistes [MASK] fait son bilan. (1)', '[MASK] spécifique sur la valorisation et le renforcement de la vitalité culturelle dans le Bas-Saint-Laurent : volet manifestations et événements culturels : fiche synthèse des projets financés (1)', 'Marseil[MASK], Saint-Ouen, Lyon : pour [MASK] Printemps du Québec, nos artistes font [MASK] tour de la France (1)', \"[MASK] votre visite|Expositions|Activités|Collections et recherche|Dons et prêts d'objets|Mon McCord\", '[MASK] votre espace personnel (décompte propriétaire, décompte de charges, quittance de loyer....)', 'Contactez notre service dédié spécifiquement aux commerces, locaux et entreprises [MASK] votre projet professionnel.', '[MASK] proposons des biens immobiliers dans le secteur de la Manche, de la Basse Normandie et du Calvados.', \"Si vous souhaitez plus d'informations [MASK] un bien immobilier à acheter ou à louer en Normandie, contactez-nous\", \"Accueil [MASK] Catalogue [MASK] JEUX D'EXTÉRIEUR & D'ADRESSE [MASK] Dextérité [MASK] TOY-63970 [MASK] Critiques Mon compte | Voir panier | Commander\", \"avec l'extension on [MASK] encore faire plus de choses et on s'eclate deux fois plus.depuis que je con..\", \"[MASK] eu l'occasion de jouer avec et il est vraiment delirant. c'est meme un des jeux preferes de mon..\", \"La Petite Souris, la Fraise bien Mûre et l'Ours [MASK] par Emilie Pirotton Date d'ajout : samedi 05 avril 2008\", 'Je suis institutri[MASK] maternelle de formation et je peux vous affirmer que [MASK] livre a énormément de s..', 'Contacts recherchés : Télévision linéaire, télévision mobile, éditeur de contenus web, [MASK] privés, [MASK] institutionnels.', 'Le principe de la série repose sur la satyre de ce qui «conditionne» le monde : idoles, icônes, monstres, actualités insolites, spectaculaires, politiques, [MASK] et autres banalités.', \"La désinvolture de Bubbleman, va l'entraîner dans des aventures drôles, poétiques, fantaisistes et scandaleuses, dans un esprit [MASK] funk, rock.\", '- Série [MASK] 11 épiso[MASK]s linéaires [MASK] 15 min. pour la télévision, internet et la télévision mobile, et une panoplie [MASK] formats très courts : trailers, accroches, gags...', \"abat-jour en chanvre et serviettes anciennes monogrammées [MASK] Tous les messages sur abat-jour en chanvre et serviettes anciennes monogrammées - L'ATELIER D'ELISE\", \"Tags : abat-jour [MASK] chanvre et serviettes anci[MASK]nes monogrammées, coussins [MASK] v[MASK]tes, et transat [MASK] lin de l'armée, rideaux etc...\", \"Blog hébergé par CanalBlog | Plan du site | Blog Couture et Scrapbooking créé le 14/02/2010 | Contacter l'auteur | [MASK] un abus\", 'Depuis que [MASK] réseaux sociaux sont à leur apogée, [MASK] métiers qui en découlent sont très nombreux. Parmi eux, [MASK] influenceurs, ces jeunes adultes...')]\n",
            "[('forum consacré à la cuisine, mais pas essentiellement. bavardages, humour et vie quotidienne sont au menu..', 'mes broderies, couture, , recettes, créations de moules latex, créations de plâtres maison , pâte fimo coussins home-made toutes les belles choses de la vie.', \"il y a bien longtemps que je n'ai pas publié, ma copinaute rosa ici m'a dit, mais tu ne publies plus rien\", \"rosa m'a refait une jolie bannière, alors pour la mettre à l'honneur et la remercier je reviens vous faire un petit coucou\", \"c'est vrai depuis trois mois j'ai attaqué à faire du sport, l'aquabikking, c'est super et çà fait un bien fou\", 'et avec ma voisine carene elle aimait bien mes créations donc elle est venue le mercredi après-midi avec celia pour apprendre', 'à vrai dire je suis assez scotchée car ni celia ni carène ne savaient coudre et elles se mont installées devant ma machine à coudre avec une aisance hors du commun, bravo.', \"concernant mes créations j'ai fait une housse de couette pour celia ma foi pour la première assez contente de moi\", \"on a profité du mois de janvier pour fêter l'épiphanie et tous les mercredi après-midi on a tiré les rois\", \"et puis envie de gaufres, donc j'ai investi dans un appareil à gaufres et voila le résultat, trop bonnes\", \"cet après-midi, j'ai organisé un petit atelier cartes de noël et avec carene et celia nous avons confectionné les cartes pour antony\", \"et puis on approche des fêtes de nooêl et cette année j'ai commencer à confectionner les petites bottes de noël\", \"et puis j'ai joué au jeu des 10 erreurs chez notre copinaute sylvie , pour son anniblog, et je n'avais encore pas relayé l'info,\", 'ce billet pour vous raconter que le 8 jullet de cette année le compteur a tourné à .... le 5 est passé à 6 et le 9 à zero.', \"je ne vous cache pas que si jusqu'à présent cela ne me faisait rien, eh bien la !!! çà m'a fait quelque chose\", \"je ne peux pas dire que l'automne et l'hiver soient mes saisons préférées, je préfére de loin l'été et le printemps,\", \"mais le calendrier a quatre saisons, et au fil des ans je m'applique à aimer ces dernières avec leurs cotés positifs.\", \"et puis le même week-end, ce qui est quand même un peu regrettable, la journée du patrimoine, assez difficile d'être présente des deux côtés\", \"du fait de la journée du patrimoine, l'association des masques de venise, les figurants déambulaient dans le château et se promenaient dans le parc\", \"et puis vite vite, il fallait retourner à l'aire d'atterrisage des parapentes, car une goutte humaine avait lieue\", \"pour ma part j'ai passé l'été entre mer et montagne, je vais vous mettre quelques images pour vous faire partager mes promenades de l'été 2014\", \"coucou tout le monde un petit article sur mon blog pour vous presenter la ronde que lylou-anne ici nous a propossé pour l'été a l'honneur des coquillages\", \"j'ai une grande collection de coquillages que je disperse sur les plages du monde. peut être l'avez-vous vue.\", \"elle est venue nous voir mercredi, c'est pat, ici, il ne faisait pas super beau, mais notre joie et notre bonne humeur étaient au beau fixe,\", 'elle est repartie avec quelques cadeaux et quelques créations home made, nouvelle collection été 2014', \"aujourd'hui nous nous rendons chez pat, dans son salon, salon que j'ai vu en vrai, lors de notre rencontre du 7 mai dernier.\", \"ensuite ma maman est venue le rejoindre, et ils n'ont plus quitté la france, et nous sommes nées en france avec ma petite soeur.\", \"le mont titano ou mont titan (monte titano en italien) est un relief montagneux des apennins qui s'élève à 750 mètres.\", \"blog hébergé par canalblog | plan du site | blog couture et scrapbooking créé le 10/05/2011 | contacter l'auteur | signaler un abus\", 'tu cherches des informations, des avis ou des personnes avec qui discuter de ta passion? tu es au bon endroit! inscris-toi vite ^^'), ('[MASK] consacré à la cuisine, mais pas essentiellement. Bavardages, humour et vie quotidienne sont au menu..', 'mes broderies, couture, , recettes, créations de moules latex, créations de plâtres maison , pâte fimo coussins home-made toutes les belles choses de la [MASK]', \"il y a bien longtemps que je n'ai pas [MASK] ma copinaute ROSA ICI m'a dit, mais tu ne publies plus rien\", \"ROSA m'a refait une [MASK] bannière, alors pour la mettre à l'honneur et la remercier je reviens vous faire un petit coucou\", \"[MASK] vrai depuis trois mois j'ai attaqué à faire du sport, l'aquabikking, [MASK] super et çà fait un bien fou\", 'et avec ma voisine CARENE elle aimait bien mes créations donc elle est venue le mercredi après-midi avec Celia [MASK] apprendre', 'à vrai dire je suis assez scotchée car ni Celia ni Carène ne [MASK] coudre et elles se mont installées devant ma machine à coudre avec une aisance hors du commun, bravo.', \"concernant mes créations j'ai fait une housse [MASK] couette pour CELIA ma foi pour la première assez contente [MASK] moi\", \"on a profité du mois de janvier pour fêter l'épiphanie et tous [MASK] mercredi après-midi on a tiré [MASK] rois\", \"et puis [MASK] de gaufres, donc j'ai investi dans un appareil à gaufres et voila le résultat, trop bonnes\", \"cet après-midi, j'ai organisé [MASK] petit atelier cartes de noël et avec CARENE et CELIA nous avons confectionné les cartes pour ANTONY\", \"et puis on approche [MASK]s fêtes [MASK] nOoêl et cette année j'ai commencer à confectionner les petites bottes [MASK] Noël\", \"et puis j'ai joué au jeu des 10 erreurs chez notre copinaute sylvie , pour son anniblog, et je n'avais encore pas relayé [MASK]\", 'ce billet pour vous raconter que le 8 JULLET de cette année le [MASK] a tourné à .... le 5 est passé à 6 et le 9 à ZERO.', \"Je ne vous cache pas que si jusqu'à présent cela ne me faisait rien, eh [MASK] la !!! çà m'a fait quelque chose\", \"Je ne peux pas dire que l'automne et [MASK] soient mes saisons préférées, je préfére de loin l'été et le printemps,\", \"mais le calendrier a [MASK] saisons, et au fil des ans je m'applique à aimer ces dernières avec leurs cotés positifs.\", \"et puis le même week-end, ce qui est quand même un peu regrettable, la journée du patrimoine, assez [MASK] d'être présente des deux côtés\", \"du [MASK] de la journée du patrimoine, l'association des masques de Venise, les figurants déambulaient dans le château et se promenaient dans le parc\", \"et puis vite [MASK] il fallait retourner à l'aire d'atterrisage des parapentes, car une goutte humaine avait lieue\", \"Pour ma part j'ai passé l'été entre mer et [MASK] je vais vous mettre quelques images pour vous faire partager mes promenades de l'été 2014\", \"Coucou tout le [MASK] un petit article sur mon blog pour vous presenter la ronde que Lylou-Anne ICI nous a propossé pour l'été a l'honneur des COQUILLAGES\", \"[MASK] une grande collection de coquillages que je disperse sur les plages du monde. Peut être l'avez-vous vue.\", \"elle est [MASK] nous voir mercredi, c'est Pat, ICI, il ne faisait pas super beau, mais notre joie et notre bonne humeur étaient au beau fixe,\", 'elle est repartie avec [MASK] cadeaux et [MASK] créations home made, nouvelle collection été 2014', \"aujourd'hui [MASK] [MASK] rendons chez PAT, dans son salon, salon que j'ai vu en vrai, lors de notre rencontre du 7 MAI dernier.\", \"Ensuite [MASK] [MASK][MASK]n est venue le rejoindre, et ils n'ont plus quitté la France, et nous sommes nées en France avec [MASK] petite soeur.\", \"Le mont Titano ou mont Titan (Monte Titano en italien) est un relief montagneux [MASK] Apennins qui s'élève à 750 mètres.\", \"Blog hébergé par CanalBlog [MASK] Plan du site [MASK] Blog Couture et Scrapbooking créé le 10/05/2011 [MASK] Contacter l'auteur [MASK] Signaler un abus\", '[MASK] cherches des informations, des avis ou des personnes avec qui discuter de ta passion? [MASK] es au bon endroit! Inscris-toi vite ^^')]\n"
          ]
        }
      ],
      "source": [
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    if batch_num > 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XnanjzqtzQi8"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=kannada_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_saWU5QmVem2"
      },
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, kn_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdgtTSKvwN9_"
      },
      "source": [
        "Modify mask such that the padding tokens cannot look ahead.\n",
        "In Encoder, tokens before it should be -1e9 while tokens after it should be -inf.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLcXI4wkMLck"
      },
      "source": [
        "Note the target mask starts with 2 rows of non masked items: https://github.com/SamLynnEvans/Transformer/blob/master/Beam.py#L55\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju59VDGLuOqf",
        "outputId": "0ad34e31-521a-4ca2-f444-26b5374946f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask \u001b[38;5;241m=\u001b[39m create_masks(eng_batch, kn_batch)\n\u001b[0;32m     13\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m kn_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkn_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_end_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_end_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m labels \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39msentence_embedding\u001b[38;5;241m.\u001b[39mbatch_tokenize(kn_batch, start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterian(\n\u001b[0;32m     25\u001b[0m     kn_predictions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kn_vocab_size)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     26\u001b[0m     labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32md:\\conda\\envs\\robso\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\conda\\envs\\robso\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[1], line 301\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[0;32m    292\u001b[0m             x, \n\u001b[0;32m    293\u001b[0m             y, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    299\u001b[0m             dec_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# We should make this true\u001b[39;00m\n\u001b[0;32m    300\u001b[0m             dec_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m): \u001b[38;5;66;03m# x, y are batch of sentences\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_start_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_end_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token\u001b[38;5;241m=\u001b[39mdec_start_token, end_token\u001b[38;5;241m=\u001b[39mdec_end_token)\n\u001b[0;32m    303\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n",
            "File \u001b[1;32md:\\conda\\envs\\robso\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\conda\\envs\\robso\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[1], line 178\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, self_attention_mask, start_token, end_token)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, self_attention_mask, start_token, end_token):\n\u001b[1;32m--> 178\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x, self_attention_mask)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32md:\\conda\\envs\\robso\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\conda\\envs\\robso\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mSentenceEmbedding.forward\u001b[1;34m(self, x, start_token, end_token)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, start_token, end_token): \u001b[38;5;66;03m# sentence\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_tokenize(x, start_token, end_token)\n\u001b[1;32m---> 71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_encoder()\u001b[38;5;241m.\u001b[39mto(get_device())\n\u001b[0;32m     73\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x \u001b[38;5;241m+\u001b[39m pos)\n",
            "File \u001b[1;32md:\\conda\\envs\\robso\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\conda\\envs\\robso\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\conda\\envs\\robso\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\conda\\envs\\robso\\Lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "transformer.train()\n",
        "transformer.to(device)\n",
        "total_loss = 0\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    iterator = iter(train_loader)\n",
        "    for batch_num, batch in enumerate(iterator):\n",
        "        transformer.train()\n",
        "        eng_batch, kn_batch = batch\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)\n",
        "        optim.zero_grad()\n",
        "        kn_predictions = transformer(eng_batch,\n",
        "                                     kn_batch,\n",
        "                                     encoder_self_attention_mask.to(device), \n",
        "                                     decoder_self_attention_mask.to(device), \n",
        "                                     decoder_cross_attention_mask.to(device),\n",
        "                                     enc_start_token=False,\n",
        "                                     enc_end_token=False,\n",
        "                                     dec_start_token=True,\n",
        "                                     dec_end_token=True)\n",
        "        labels = transformer.decoder.sentence_embedding.batch_tokenize(kn_batch, start_token=False, end_token=True)\n",
        "        loss = criterian(\n",
        "            kn_predictions.view(-1, kn_vocab_size).to(device),\n",
        "            labels.view(-1).to(device)\n",
        "        ).to(device)\n",
        "        valid_indicies = torch.where(labels.view(-1) == kannada_to_index[PADDING_TOKEN], False, True)\n",
        "        loss = loss.sum() / valid_indicies.sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        #train_losses.append(loss.item())\n",
        "        if batch_num % 100 == 0:\n",
        "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
        "            print(f\"English: {eng_batch[0]}\")\n",
        "            print(f\"Kannada Translation: {kn_batch[0]}\")\n",
        "            kn_sentence_predicted = torch.argmax(kn_predictions[0], axis=1)\n",
        "            predicted_sentence = \"\"\n",
        "            for idx in kn_sentence_predicted:\n",
        "              if idx == kannada_to_index[END_TOKEN]:\n",
        "                break\n",
        "              predicted_sentence += index_to_kannada[idx.item()]\n",
        "            print(f\"Kannada Prediction: {predicted_sentence}\")\n",
        "\n",
        "\n",
        "            transformer.eval()\n",
        "            kn_sentence = (\"\",)\n",
        "            eng_sentence = (\"should we go to the mall?\",)\n",
        "            for word_counter in range(max_sequence_length):\n",
        "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
        "                predictions = transformer(eng_sentence,\n",
        "                                          kn_sentence,\n",
        "                                          encoder_self_attention_mask.to(device), \n",
        "                                          decoder_self_attention_mask.to(device), \n",
        "                                          decoder_cross_attention_mask.to(device),\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
        "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "                next_token = index_to_kannada[next_token_index]\n",
        "                kn_sentence = (kn_sentence[0] + next_token, )\n",
        "                if next_token == END_TOKEN:\n",
        "                  break\n",
        "            \n",
        "            print(f\"Evaluation translation (should we go to the mall?) : {kn_sentence}\")\n",
        "            print(\"-------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nosVPGVijId"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOQe-juylBiJ"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "def translate(eng_sentence):\n",
        "  eng_sentence = (eng_sentence,)\n",
        "  kn_sentence = (\"\",)\n",
        "  for word_counter in range(max_sequence_length):\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
        "    predictions = transformer(eng_sentence,\n",
        "                              kn_sentence,\n",
        "                              encoder_self_attention_mask.to(device), \n",
        "                              decoder_self_attention_mask.to(device), \n",
        "                              decoder_cross_attention_mask.to(device),\n",
        "                              enc_start_token=False,\n",
        "                              enc_end_token=False,\n",
        "                              dec_start_token=True,\n",
        "                              dec_end_token=False)\n",
        "    next_token_prob_distribution = predictions[0][word_counter]\n",
        "    next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "    next_token = index_to_kannada[next_token_index]\n",
        "    kn_sentence = (kn_sentence[0] + next_token, )\n",
        "    if next_token == END_TOKEN:\n",
        "      break\n",
        "  return kn_sentence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDVH_YsxlK6q",
        "outputId": "83c47f99-53c0-4c2d-c26a-aaa426f50563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದರ ಬಗ್ಗೆ ಏನು ಮಾಡಬೇಕು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"what should we do when the day starts?\")\n",
        "print(translation)\n",
        "#ದಿನ ಪ್ರಾರಂಭವಾದಾಗ ನಾವು ಏನು ಮಾಡಬೇಕು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9yfawBnul0W",
        "outputId": "d9e6e6b7-683b-45f9-f013-c53c31038306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಹೇಗೆ ಇದು ಹೇಗೆ ಹೇಗೆ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"how is this the truth?\")\n",
        "print(translation)\n",
        "#ಇದು ಹೇಗೆ ಸತ್ಯ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpdYBk5-urcQ",
        "outputId": "ca7249c5-efda-4f41-f052-ecef9691be82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದರಿಂದ ಮೂಲಕ ಸಂಬಂಧಿಸಿದ ಮೇಲೆ ಮಾಡಿದ್ದಾರೆ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"the world is a large place with different people\")\n",
        "print(translation)\n",
        "#ಪ್ರಪಂಚವು ವಿಭಿನ್ನ ಜನರೊಂದಿಗೆ ದೊಡ್ಡ ಸ್ಥಳವಾಗಿದೆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni9e2UYUuxi3",
        "outputId": "b93968e6-3f12-4794-b277-3a3821af221e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ಕುಟುಂಬದ ಹೆಸರು<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"my name is ajay\")\n",
        "print(translation)\n",
        "#ನನ್ನ ಹೆಸರು ಅಜಯ್"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJuJKHqFldW3",
        "outputId": "71aa2c6c-ec77-4b02-d39b-bd724012fb53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ಅಂತರ ಸಂಗತಿ ನಾನು ಕೊಡುವುದಿಲ್ಲ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"i cannot stand this smell\")\n",
        "print(translation)\n",
        "#ನಾನು ಈ ವಾಸನೆಯನ್ನು ಸಹಿಸುವುದಿಲ್ಲ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxHC4Lirlfu8",
        "outputId": "5a3ca401-abac-41af-d9db-99fb7a57fe00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಯಾವುದೇ ಕಾರಣಗಳು ಇಲ್ಲ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"noodles are the best\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLVOSI0Oli16",
        "outputId": "9f9445a1-2802-4688-900c-d7ecf2bd5c35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಏಕೆ ಕಾರಣ ಏನು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"why care about this?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MStWCoAt0Ixp"
      },
      "source": [
        "This translated pretty well : \"What is the reason. Why\" without punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB6TEJfGlkRT",
        "outputId": "adb465d5-b0ed-4be4-9251-62c079f49491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದು ಅತ್ಯಂತ ಹೊರತಾಗಿದೆ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"this is the best thing ever\")\n",
        "print(translation)\n",
        "# ಇದು ಎಂದೆಂದಿಗೂ ಉತ್ತಮವಾಗಿದೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxsUjSybxYkh"
      },
      "source": [
        "The translation : \"This is very unusual\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQwDbuWBlmmA",
        "outputId": "7ae0e2c0-02c0-4c74-bc47-26b67da55a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ಕೇಳಿದ್ದೇನೆ.<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"i am here\")\n",
        "print(translation)\n",
        "# ನಾನು ಇಲ್ಲಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmyZ2-I6x0Yf"
      },
      "source": [
        "Translation: \"I have heard\". \n",
        "This is why word based translator may perform better than character translator. This is actually very good at optimizing the objective of the current transformer even though the translation is off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ifeV4bGluIj",
        "outputId": "6bce922d-d0db-432c-e6c6-97d482f1dea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಕ್ಯಾನ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಮಾಡಿ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"click this\")\n",
        "print(translation)\n",
        "# ಇದನ್ನು ಕ್ಲಿಕ್ ಮಾಡಿ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RB5DUBEl1kD",
        "outputId": "9109e504-8b9e-45dc-e13c-e878b3741e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಎಲ್ಲಿ ಎಲ್ಲಿ ಎಲ್ಲಿ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"where is the mall?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdeJ9CvMn5LM",
        "outputId": "044b5dac-29a9-4b60-e66a-4e10739a9756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಏನು ಮಾಡಬೇಕು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"what should we do?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoikFnov1rj-"
      },
      "source": [
        "This is correct; but it absolutely fumbles on the next one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GFeyzrg1fIZ",
        "outputId": "2b4dfb04-def2-4725-9e76-5476bf85e2ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಅದನ್ನು ಮಾಡಿದ ಮೇಲೆ ಮಾಡಿದ ಮಾಡಿದ್ದಾರೆ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"today, what should we do\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0upygLS-sXcO",
        "outputId": "53a62435-ab16-4d4c-8a9f-0bf07af2a501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಅದರ ಮೇಲೆ ಏಕೆ ಮಾಡಿದ್ದಾರೆ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"why did they activate?\")\n",
        "print(translation)\n",
        "# ಅವರು ಏಕೆ ಸಕ್ರಿಯಗೊಳಿಸಿದರು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSrsqEGmtcl2",
        "outputId": "1b8b8faa-5370-426a-f2f2-fe852696bf7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಅವರು ಏಕೆ ಅವರು ಏಕೆ ಮಾಡಿದ್ದಾರೆ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"why did they do this?\")\n",
        "print(translation)\n",
        "# ಅವರು ಇದನ್ನು ಏಕೆ ಮಾಡಿದರು?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ISM5rd3BLJ"
      },
      "source": [
        "That turned out well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTcH2HFtyld",
        "outputId": "9dea5498-f139-4cbd-ee8c-6108e1b92fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ನಾನು ಕೊಡುತ್ತೇನೆ.<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"i am well.\")\n",
        "print(translation)\n",
        "# ನಾನು ಆರಾಮವಾಗಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0YX2g74eP7"
      },
      "source": [
        "Translation: \"I will give you something\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pGdN13kt5Br",
        "outputId": "240256c5-f594-41b0-8218-2c70a22a156f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದರ ಬಗ್ಗೆ ಏನು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"whats the word on the street?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFYZ6pOe4o-X"
      },
      "source": [
        "Kind of close semantically. Translation is something like: \"What is this about\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzrOcNUk1-e5"
      },
      "source": [
        "## Insights\n",
        "\n",
        "- When training, we can treat every alphabet as a single unit instead of splitting it into it's corresponding parts to preserve meaning. For example, ಮಾ should be 1 unit when comuting a loss. It should not be decomposed into ಮ + ఆ\n",
        "- Using word-based or BPE based tokenizations may help mitigate (1). Also, we will get valid word (or BPE) units if we do so. \n",
        "- Make sure the training set has a large variety of sentences that are not just about one topic like \"work\" and \"government\"\n",
        "- Increase the number of encoder / decoder units for better translations. It was set to the minimum of 1 of each unit here.\n",
        "- Create a translator with a language you understand ideally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd6_k0Uu5V7f"
      },
      "source": [
        "Overall, this model definately learned something. And you can use other languages instead of this kannada language and might see better luck"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
